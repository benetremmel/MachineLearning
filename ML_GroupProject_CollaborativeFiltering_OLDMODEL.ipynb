{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:29:40.218908Z",
     "start_time": "2024-05-14T21:29:39.952366Z"
    }
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in /Users/leonardoheinemann/anaconda3/lib/python3.11/site-packages (0.1)\r\n",
      "Requirement already satisfied: scikit-surprise in /Users/leonardoheinemann/anaconda3/lib/python3.11/site-packages (from surprise) (1.1.3)\r\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/leonardoheinemann/anaconda3/lib/python3.11/site-packages (from scikit-surprise->surprise) (1.2.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/leonardoheinemann/anaconda3/lib/python3.11/site-packages (from scikit-surprise->surprise) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/leonardoheinemann/anaconda3/lib/python3.11/site-packages (from scikit-surprise->surprise) (1.11.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:29:44.289094Z",
     "start_time": "2024-05-14T21:29:42.152770Z"
    }
   },
   "id": "11a3b27dcab4d935",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "9f3f1ed81e5340dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622d2a2241596f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person.\n",
    "\n",
    "There are two types of collaborative filtering: user-based and item-based. User-based collaborative filtering is based on the similarity between users and item-based collaborative filtering is based on the similarity between items. For our recommender system we chose an item-based approach. The reasons for that are many. Item-based collaborative filtering is often preferred over user-based collaborative filtering, particularly in environments where the item catalog is relatively stable and doesn't grow as quickly as the user base. Item-based systems have a better scalability and efficiency, especially with large user bases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8fea516d50400",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Recapt Part 1: Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21248191d2e61fc8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To build an item-based collaborative filtering system, we need to calculate the similarity between items based on the ratings users have given to those items. We will use the cosine similarity to calculate the similarity between items. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33512e045660dbaf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:39:39.830475Z",
     "start_time": "2024-05-14T21:39:39.372200Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged = pd.read_pickle('data/df_movies_cleaned.pkl')\n",
    "df_ratings = pd.read_pickle('data/df_ratings_3M.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.drop(columns=['user_mean_rating', 'liked_by_user'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:39:40.467161Z",
     "start_time": "2024-05-14T21:39:40.433529Z"
    }
   },
   "id": "b18e922010ffc01c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3000001 entries, 11800835 to 7900459\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   userId           int64         \n",
      " 1   movieId          int64         \n",
      " 2   rating           Float64       \n",
      " 3   timestamp        datetime64[ns]\n",
      " 4   rating_category  category      \n",
      "dtypes: Float64(1), category(1), datetime64[ns](1), int64(2)\n",
      "memory usage: 120.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_ratings.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:57:03.214372Z",
     "start_time": "2024-05-14T21:57:03.202309Z"
    }
   },
   "id": "8fc9cdaff0039294",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4851b0a6f34b61d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:39:40.952406Z",
     "start_time": "2024-05-14T21:39:40.753742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             userId       movieId     rating                      timestamp\ncount  3.000001e+06  3.000001e+06  3000001.0                        3000001\nmean   1.351007e+05  1.615902e+04   3.528979  2007-02-18 20:57:45.565002496\nmin    1.000000e+00  1.000000e+00        0.5            1996-01-29 00:00:00\n25%    6.735200e+04  1.088000e+03        3.0            2001-06-07 13:51:18\n50%    1.353090e+05  2.657000e+03        3.5            2006-06-12 15:55:57\n75%    2.027130e+05  6.707000e+03        4.0            2013-02-11 17:27:34\nmax    2.708960e+05  1.762670e+05        5.0            2017-08-04 06:57:50\nstd    7.814670e+04  3.127131e+04   1.060178                            NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3.000001e+06</td>\n      <td>3.000001e+06</td>\n      <td>3000001.0</td>\n      <td>3000001</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.351007e+05</td>\n      <td>1.615902e+04</td>\n      <td>3.528979</td>\n      <td>2007-02-18 20:57:45.565002496</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.5</td>\n      <td>1996-01-29 00:00:00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.735200e+04</td>\n      <td>1.088000e+03</td>\n      <td>3.0</td>\n      <td>2001-06-07 13:51:18</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.353090e+05</td>\n      <td>2.657000e+03</td>\n      <td>3.5</td>\n      <td>2006-06-12 15:55:57</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.027130e+05</td>\n      <td>6.707000e+03</td>\n      <td>4.0</td>\n      <td>2013-02-11 17:27:34</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.708960e+05</td>\n      <td>1.762670e+05</td>\n      <td>5.0</td>\n      <td>2017-08-04 06:57:50</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.814670e+04</td>\n      <td>3.127131e+04</td>\n      <td>1.060178</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the train/test split we will split the data randomly. We will use 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44840797c8623209"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# randome Train/Test Split\n",
    "\n",
    "train_data, test_data = train_test_split(df_ratings, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:39:41.313865Z",
     "start_time": "2024-05-14T21:39:41.114161Z"
    }
   },
   "id": "1ee56b1b9b2ce50d",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "233b79209f07a069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Singular Value Decomposition (SVD)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e5a78ccfe103a2c"
  },
  {
   "cell_type": "markdown",
   "id": "e408f10ab34450d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will use Singular Value Decomposition (SVD) to create the collaborative filtering model. SVD is a matrix factorization technique that is commonly used for recommendation systems. It decomposes the user-item interaction matrix into different matrices. SVD helps in extracting latent factors that explain observed ratings, efficiently reducing data dimensionality while preserving essential information. This significantly speeds up calculations, making the process of predicting ratings more efficient, especially when dealing with a large dataset like ours. Additionally, by focusing on these latent factors, SVD enables a deeper understanding of user preferences and item characteristics, promising more personalized and accurate recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855480f2",
   "metadata": {},
   "source": [
    "Do compute SVD, we will use the surprise library. Surprise automatically handles normalization and scaling of the data as well as the handling of cold start and sparsity issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also use a GridSearch to find the best combination of hyperparameter for the model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "772fca0e31f76bc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8913\n",
      "RMSE: 0.8829\n",
      "RMSE: 0.8978\n",
      "RMSE: 0.8840\n",
      "RMSE: 0.9182\n",
      "RMSE: 0.8831\n",
      "RMSE: 0.9193\n",
      "RMSE: 0.8844\n",
      "Best RMSE score obtained:  0.8828869256424875\n",
      "Best parameters:  {'lr_all': 0.005, 'n_epochs': 20, 'n_factors': 50, 'reg_all': 0.05}\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "data = Dataset.load_from_df(train_data[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "testset = list(zip(test_data['userId'].values, test_data['movieId'].values, test_data['rating'].values))\n",
    "\n",
    "# our grid of parameters\n",
    "param_grid = {'n_factors': [50, 100],  # Number of factors\n",
    "              'n_epochs': [20],         # Number of iterations\n",
    "              'lr_all': [0.005, 0.01],      # Learning rate\n",
    "              'reg_all': [0.02, 0.05]}      # Regularization term\n",
    "\n",
    "svd = SVD()\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Loop through parameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    svd = SVD(**params)\n",
    "    svd.fit(trainset)\n",
    "\n",
    "    predictions = svd.test(testset)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "\n",
    "    # Update best RMSE and parameters if necessary\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best RMSE score obtained: \", best_rmse)\n",
    "print(\"Best parameters: \", best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T21:48:50.658715Z",
     "start_time": "2024-05-14T21:46:23.081688Z"
    }
   },
   "id": "75b157c6c93dba85",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best RMSE score obtained is 0.88 with the following parameters: \n",
    "\n",
    "lr_all: 0.005, n_epochs: 20, n_factors: 50, reg_all: 0.05"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f5d0e127b653a8"
  },
  {
   "cell_type": "markdown",
   "id": "3e9f81f6",
   "metadata": {},
   "source": [
    "Let us now train the best version of our model and evaluate it on the test set with precision as metric."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8119\n"
     ]
    }
   ],
   "source": [
    "best_model = SVD(**best_params)\n",
    "best_model.fit(trainset)\n",
    "\n",
    "# Test the final model\n",
    "final_predictions = best_model.test(testset)\n",
    "\n",
    "# Function to calculate precision metric\n",
    "def calculate_precision(predictions, threshold=3.5):\n",
    "    true_positives = 0\n",
    "    predicted_positives = 0\n",
    "\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        if est >= threshold:\n",
    "            predicted_positives += 1\n",
    "            if true_r >= threshold:\n",
    "                true_positives += 1\n",
    "\n",
    "    if predicted_positives == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = true_positives / predicted_positives\n",
    "    return precision\n",
    "\n",
    "# Calculate precision\n",
    "precision = calculate_precision(final_predictions, threshold=4.0)\n",
    "print(f\"Precision: {precision:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T19:19:21.910283Z",
     "start_time": "2024-05-15T19:19:06.390111Z"
    }
   },
   "id": "279b8cf5b4261c62",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e651c0227f1602e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
