{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import scipy.stats\n",
    "from scipy.sparse.linalg import svds\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from scipy.sparse import csr_matrix\n",
    "from surprise import accuracy\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_pickle('data/df_movies_cleaned.pkl')\n",
    "df_ratings = pd.read_pickle('data/df_ratings_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual Feature - Combined Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['combined_text'] = df_merged.apply(lambda row: ' '.join([\n",
    "    ' '.join(row['genre_extracted']), \n",
    "    ' '.join(row['actors']), \n",
    "    ' '.join(row['keywords_extracted']), \n",
    "    row['overview'], \n",
    "    ' '.join(row['production_company_extracted'])\n",
    "]).lower(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined_text feature aggregates critical textual metadata from genres, actors, keywords, and movie descriptions into a single comprehensive descriptor for each movie. This aggregation captures the essence of a movie’s content, thematic elements, and appeal, which is crucial for content-based filtering. By synthesizing this information, the recommender system can identify and suggest movies with similar thematic and content attributes, enhancing personalization and user engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining df_ratings and df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.merge(df_ratings, df_merged, on='movieId', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Rating Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to set a threshold of 20 ratings for each movie before including it in the item-based recommender system is strategic, with the goal of ensuring the reliability and validity of the generated recommendations. This threshold acts as a quality control measure, weeding out movies with sparse feedback that could otherwise result in skewed or less confident recommendations due to insufficient user data. By setting this minimum, the system focuses on movies with a high level of viewer engagement, allowing recommendations to be built on a solid foundation of user feedback. This approach improves the system's ability to deliver accurate, trustworthy recommendations based on broad consensus rather than outliers or minimal feedback, resulting in a better user experience and increased overall credibility for the recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: (24669326, 19)\n",
      "Filtered dataset size: (24548423, 19)\n"
     ]
    }
   ],
   "source": [
    "ratings_per_movie = df_combined.groupby('movieId').size()\n",
    "\n",
    "movies_with_enough_ratings = ratings_per_movie[ratings_per_movie >= 20].index\n",
    "\n",
    "df_item_modeling = df_combined[df_combined['movieId'].isin(movies_with_enough_ratings)]\n",
    "\n",
    "print(f\"Original dataset size: {df_combined.shape}\")\n",
    "print(f\"Filtered dataset size: {df_item_modeling.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the filtered dataset, df_item_modeling, now comprising 24.548.423 rows out of the original 24.669.326, it's evident that the vast majority of the data meets the threshold of having at least 20 ratings per movie. This minimal reduction in dataset size suggests that most movies in the dataset have a sufficient number of ratings, indicating robust user engagement across a wide range of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>animation comedy family tom hanks tim allen do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>adventure fantasy family robin williams jonath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>romance comedy walter matthau jack lemmon ann-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>comedy drama romance whitney houston angela ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>comedy steve martin diane keaton martin short ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                        title  \\\n",
       "0        1                    Toy Story   \n",
       "1        2                      Jumanji   \n",
       "2        3             Grumpier Old Men   \n",
       "3        4            Waiting to Exhale   \n",
       "4        5  Father of the Bride Part II   \n",
       "\n",
       "                                       combined_text  \n",
       "0  animation comedy family tom hanks tim allen do...  \n",
       "1  adventure fantasy family robin williams jonath...  \n",
       "2  romance comedy walter matthau jack lemmon ann-...  \n",
       "3  comedy drama romance whitney houston angela ba...  \n",
       "4  comedy steve martin diane keaton martin short ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped = df_item_modeling.groupby('movieId', as_index=False).agg({\n",
    "    'title': 'first',\n",
    "    'combined_text': 'first', \n",
    "})\n",
    "\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the combined_text using TF-IDF transforms qualitative textual information into quantitative vectors, facilitating the measurement of content similarity between movies. This numerical representation allows for sophisticated algorithms to compute similarities based on thematic elements, narrative structures, and genre affiliations. For our movie recommender system, this means being able to recommend movies that are contextually and thematically aligned with a user’s preferences, enhancing the discovery of relevant and appealing content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineContentRecommender:\n",
    "    def __init__(self, movies_df, k=100):\n",
    "        self.movies_df = movies_df.copy()\n",
    "        self.movies_df['movieId'] = self.movies_df['movieId'].astype(str)\n",
    "        self.movie_id_to_index = {movie_id: i for i, movie_id in enumerate(self.movies_df['movieId'])}\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(df_grouped['combined_text'])\n",
    "        self.similarity_matrix = cosine_similarity(self.tfidf_matrix)\n",
    "\n",
    "    def recommend(self, movie_id, top_n=10):\n",
    "        movie_id = str(movie_id)\n",
    "        if movie_id not in self.movie_id_to_index:\n",
    "            print(f\"Movie ID {movie_id} not found in the dataset.\")\n",
    "            return []\n",
    "        \n",
    "        movie_index = self.movie_id_to_index[movie_id]\n",
    "        similarity_scores = self.similarity_matrix[movie_index]\n",
    "        top_k_indices = np.argsort(similarity_scores)[::-1][1:top_n+1]\n",
    "        recommendations = self.movies_df.iloc[top_k_indices].copy()\n",
    "        recommendations['cosine_similarity'] = similarity_scores[top_k_indices]\n",
    "        \n",
    "        return recommendations.sort_values('cosine_similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId            title  cosine_similarity\n",
      "2874     3114      Toy Story 2           0.498092\n",
      "12007   78499      Toy Story 3           0.417211\n",
      "1722     1920   Small Soldiers           0.216866\n",
      "2048     2253             Toys           0.186901\n",
      "7112     7987            Dolls           0.180138\n",
      "12339   83219  The Pixar Story           0.178043\n",
      "1552     1707     Home Alone 3           0.163932\n",
      "1793     1991     Child's Play           0.151260\n",
      "9645    46948    Monster House           0.144818\n",
      "1795     1993   Child's Play 3           0.143198\n"
     ]
    }
   ],
   "source": [
    "recommender_base = BaselineContentRecommender(df_grouped, k=100)\n",
    "recommendations_base1 = recommender_base.recommend('1', top_n=10)  \n",
    "print(recommendations_base1[['movieId', 'title', 'cosine_similarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId                             title  cosine_similarity\n",
      "8558    27884                         Word Wars           0.246909\n",
      "5855     6304                         Brainscan           0.164207\n",
      "9503    44731                        Stay Alive           0.144236\n",
      "14410  113889  Angry Video Game Nerd: The Movie           0.143081\n",
      "9366    42725                     Grandma's Boy           0.142472\n",
      "13344   97913                    Wreck-It Ralph           0.139435\n",
      "7422     8633              The Last Starfighter           0.132591\n",
      "15323  139847                         Chevalier           0.131599\n",
      "8295    26985                           Nirvana           0.130764\n",
      "14499  115534                             Ouija           0.127464\n"
     ]
    }
   ],
   "source": [
    "recommendations_base2 = recommender_base.recommend('2', top_n=10)  \n",
    "print(recommendations_base2[['movieId', 'title', 'cosine_similarity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized combined_text, which includes genres, keywords, and other descriptive elements, captures the thematic essence of films. This feature uses TF-IDF vectors to emphasize unique descriptors, allowing for the recommendation of movies with similar thematic and stylistic content. This is critical for a content-based system that relies on content similarities. This feature, as already used for the baseline-model, is critical and is shaping the foundation of our modeling approach. However, in order to advance the model we are increasing complexity by adding additonal features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "Feature engineering is an important step in the development of machine learning models, including recommender systems, because it involves extracting meaningful variables from raw data to improve model performance and accuracy. This process transforms complex and often unstructured information into structured, analytically useful formats, allowing models to uncover previously unknown patterns, relationships, and insights. In the context of developing a movie recommendation system, effective feature engineering ensures that the nuances of movie content, user preferences, and contextual factors are accurately captured and used. By carefully selecting, combining, and transforming data into features such as weighted scores, combined textual data, and sentiment analysis, developers can significantly improve the system's ability to provide personalized, relevant, and appealing movie recommendations. This not only improves user satisfaction and engagement, but it also strengthens the business case by increasing platform usage and retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating           timestamp  user_mean_rating  \\\n",
      "0       1      110     1.0 2015-03-09 22:52:09          4.277778   \n",
      "1       1      147     4.5 2015-03-09 23:07:15          4.277778   \n",
      "2       1      858     5.0 2015-03-09 22:52:03          4.277778   \n",
      "3       1     1221     5.0 2015-03-09 22:52:26          4.277778   \n",
      "4       1     1246     5.0 2015-03-09 22:52:36          4.277778   \n",
      "\n",
      "   liked_by_user  weighted_score  average_rating  rating_count  \n",
      "0          False        4.000352        4.010725         62332  \n",
      "1           True        3.513227        3.581926          4559  \n",
      "2           True        4.319932        4.336495         52237  \n",
      "3           True        4.238059        4.261745         34163  \n",
      "4           True        3.888786        3.911582         25012  \n"
     ]
    }
   ],
   "source": [
    "movie_stats = df_ratings.groupby('movieId').agg(average_rating=('rating', 'mean'), rating_count=('rating', 'count')).reset_index()\n",
    "\n",
    "C = movie_stats['average_rating'].mean()\n",
    "m = movie_stats['rating_count'].quantile(0.90)\n",
    "\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['rating_count']  \n",
    "    R = x['average_rating'] \n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "movie_stats['weighted_score'] = movie_stats.apply(weighted_rating, axis=1)\n",
    "\n",
    "df_ratings = df_ratings.merge(movie_stats[['movieId', 'weighted_score']], on='movieId', how='left')\n",
    "\n",
    "df_ratings = df_ratings.merge(movie_stats[['movieId', 'average_rating', 'rating_count']], on='movieId', how='left')\n",
    "\n",
    "print(df_ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted score combines a movie's average rating (vote_average) and the number of ratings (vote_count) it has received to provide a balanced metric that reflects both popularity and quality. This approach mitigates the bias towards movies with a high average rating but a low number of ratings, ensuring that the recommendations are not only high-quality but also broadly appreciated. For a movie recommender system, integrating the weighted score helps prioritize movies that have proven appeal, aligning recommendations with broader viewer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "df_merged['movie_age'] = current_year - pd.to_datetime(df_merged['release_date']).dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the movie_age from the release date provides insight into the recency and potential cultural relevance of a movie. In the context of a movie recommender system, this allows for temporal filtering and trend analysis, enabling recommendations that cater to preferences for newer releases or classic films. Understanding movie age is essential for aligning recommendations with temporal viewing trends and user preferences for contemporary versus classic cinema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis of Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return None \n",
    "\n",
    "df_merged['sentiment_polarity'] = df_merged['overview'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing sentiment analysis on movie descriptions yields a sentiment_polarity score, offering a nuanced view of the emotional tone or mood conveyed by the movie's narrative. This feature is particularly important for recommending movies that match a user’s emotional preferences or current mood, adding an additional layer of personalization. By integrating sentiment analysis, your recommender system can differentiate movies not just by genre or content but also by the emotional experience they offer, enhancing user satisfaction and engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding new features to df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_aggregated = df_ratings.groupby('movieId', as_index=False).agg({\n",
    "    'weighted_score': 'mean'  \n",
    "})\n",
    "\n",
    "df_merged_aggregated = df_merged.groupby('movieId', as_index=False).agg({\n",
    "    'movie_age': 'mean', \n",
    "    'sentiment_polarity': 'mean'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine df_ratings and df_merged to simplify our dataset, ensuring that each movieId is represented by a single set of features. By averaging weighted_score, movie_age, and sentiment_polarity, we capture each film's overall essence, reflecting collective attributes and sentiments. This preprocessing step converts our data into a unified df_grouped format, with each movie listed uniquely, simplifying subsequent analyses and modeling efforts. This approach not only consolidates our dataset to improve efficiency, but it also aligns with our goal of building a cohesive and analytically robust foundation for our recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_grouped.merge(df_ratings_aggregated, on='movieId', how='left')\n",
    "df_grouped = df_grouped.merge(df_merged_aggregated, on='movieId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>weighted_score</th>\n",
       "      <th>movie_age</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>animation comedy family tom hanks tim allen do...</td>\n",
       "      <td>3.884349</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.112121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>adventure fantasy family robin williams jonath...</td>\n",
       "      <td>3.227394</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>romance comedy walter matthau jack lemmon ann-...</td>\n",
       "      <td>3.141024</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.038889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>comedy drama romance whitney houston angela ba...</td>\n",
       "      <td>2.895332</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>comedy steve martin diane keaton martin short ...</td>\n",
       "      <td>3.061166</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16122</th>\n",
       "      <td>173941</td>\n",
       "      <td>Atomic Blonde</td>\n",
       "      <td>action thriller charlize theron james mcavoy s...</td>\n",
       "      <td>3.067902</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16123</th>\n",
       "      <td>174053</td>\n",
       "      <td>Black Mirror: White Christmas</td>\n",
       "      <td>drama horror mystery science fiction thriller ...</td>\n",
       "      <td>3.149876</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.067143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16124</th>\n",
       "      <td>174055</td>\n",
       "      <td>Dunkirk</td>\n",
       "      <td>action drama history thriller war fionn whiteh...</td>\n",
       "      <td>3.344268</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16125</th>\n",
       "      <td>174371</td>\n",
       "      <td>Once Upon a Time in Venice</td>\n",
       "      <td>action comedy thriller bruce willis jason momo...</td>\n",
       "      <td>3.026085</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16126</th>\n",
       "      <td>174585</td>\n",
       "      <td>Transformers: The Last Knight</td>\n",
       "      <td>action science fiction thriller adventure mark...</td>\n",
       "      <td>2.929636</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.104167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16127 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                          title  \\\n",
       "0            1                      Toy Story   \n",
       "1            2                        Jumanji   \n",
       "2            3               Grumpier Old Men   \n",
       "3            4              Waiting to Exhale   \n",
       "4            5    Father of the Bride Part II   \n",
       "...        ...                            ...   \n",
       "16122   173941                  Atomic Blonde   \n",
       "16123   174053  Black Mirror: White Christmas   \n",
       "16124   174055                        Dunkirk   \n",
       "16125   174371     Once Upon a Time in Venice   \n",
       "16126   174585  Transformers: The Last Knight   \n",
       "\n",
       "                                           combined_text  weighted_score  \\\n",
       "0      animation comedy family tom hanks tim allen do...        3.884349   \n",
       "1      adventure fantasy family robin williams jonath...        3.227394   \n",
       "2      romance comedy walter matthau jack lemmon ann-...        3.141024   \n",
       "3      comedy drama romance whitney houston angela ba...        2.895332   \n",
       "4      comedy steve martin diane keaton martin short ...        3.061166   \n",
       "...                                                  ...             ...   \n",
       "16122  action thriller charlize theron james mcavoy s...        3.067902   \n",
       "16123  drama horror mystery science fiction thriller ...        3.149876   \n",
       "16124  action drama history thriller war fionn whiteh...        3.344268   \n",
       "16125  action comedy thriller bruce willis jason momo...        3.026085   \n",
       "16126  action science fiction thriller adventure mark...        2.929636   \n",
       "\n",
       "       movie_age  sentiment_polarity  \n",
       "0           29.0            0.112121  \n",
       "1           29.0           -0.218750  \n",
       "2           29.0            0.038889  \n",
       "3           29.0            0.600000  \n",
       "4           29.0            0.466667  \n",
       "...          ...                 ...  \n",
       "16122        7.0           -0.266667  \n",
       "16123       10.0            0.067143  \n",
       "16124        7.0            0.000000  \n",
       "16125        7.0            0.075000  \n",
       "16126        7.0           -0.104167  \n",
       "\n",
       "[16127 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When developing a content-based recommender system, the selection of features is critical to its success. The selected features - weighted_score, vectorized combined_text, movie_age, runtime, and sentiment_polarity - are critical for capturing the multifaceted nature of films and their reception by audiences.\n",
    "\n",
    "The weighted_score is critical for determining a movie's appeal, as it combines the average rating with the number of ratings to provide a balanced picture of its popularity and acceptance. This feature helps to reduce biases toward movies with less ratings, ensuring that recommendations are not only popular but also well-regarded.\n",
    "\n",
    "Movie_age incorporate personal preference and temporal relevance into the recommendation process.This feature allows the system to align recommendations with users' inclinations towards newer releases or classic films. These features provide additional layers of personalization, increasing user satisfaction by accommodating individual preferences for movie duration and novelty.\n",
    "\n",
    "Finally, sentiment_polarity provides information about the emotional tone of movie descriptions or reviews. This feature allows the system to recommend movies that match not only in content but also in mood, providing a more nuanced approach to similarity that goes beyond simple thematic alignment.\n",
    "\n",
    "Together, these features form a strong foundation for an item-based recommender system. By taking into account both content and key characteristics that influence viewer preferences, the system is better able to provide precise and satisfying movie recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedContentRecommender:\n",
    "    def __init__(self, movies_df, k=100):\n",
    "        self.movies_df = movies_df.copy()\n",
    "        self.movies_df['movieId'] = self.movies_df['movieId'].astype(str)\n",
    "        self.k = k\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.movies_df['combined_text'])\n",
    "        self.similarity_matrix = cosine_similarity(self.tfidf_matrix)\n",
    "\n",
    "    def recommend(self, movie_id, top_n=10):\n",
    "        movie_id = str(movie_id)\n",
    "        if movie_id not in self.movies_df['movieId'].values:\n",
    "            print(f\"Movie ID {movie_id} not found in the dataset.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        movie_index = self.movies_df.index[self.movies_df['movieId'] == movie_id].tolist()[0]\n",
    "        similarity_scores = self.similarity_matrix[movie_index]\n",
    "        top_k_indices = np.argsort(similarity_scores)[::-1][1:self.k+1]\n",
    "        top_k_df = self.movies_df.iloc[top_k_indices].copy()\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        for feature in ['weighted_score', 'movie_age']:\n",
    "            if feature in top_k_df:\n",
    "                top_k_df[feature] = scaler.fit_transform(top_k_df[[feature]].values.reshape(-1, 1))\n",
    "\n",
    "        top_k_df['cosine_similarity'] = similarity_scores[top_k_indices]\n",
    "        top_k_df['combined_score'] = (\n",
    "            0.5 * top_k_df['cosine_similarity'] +\n",
    "            0.2 * top_k_df['weighted_score'] + \n",
    "            0.2 * top_k_df['sentiment_polarity'] +\n",
    "            0.1 * top_k_df['movie_age']\n",
    "        )\n",
    "        \n",
    "        return top_k_df.nlargest(top_n, 'combined_score')[['movieId', 'title', 'combined_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId                  title  combined_score\n",
      "2874     3114            Toy Story 2        0.515295\n",
      "12007   78499            Toy Story 3        0.378643\n",
      "5555     5974    The Thief of Bagdad        0.355765\n",
      "1130     1230             Annie Hall        0.343074\n",
      "7112     7987                  Dolls        0.323468\n",
      "1143     1244              Manhattan        0.315088\n",
      "4019     4339     Von Ryan's Express        0.314689\n",
      "2142     2355           A Bug's Life        0.313347\n",
      "576       596              Pinocchio        0.308571\n",
      "1024     1103  Rebel Without a Cause        0.302987\n"
     ]
    }
   ],
   "source": [
    "recommender_advanced = AdvancedContentRecommender(df_grouped, k=100)\n",
    "recommendations_advanced1 = recommender_advanced.recommend('1', top_n=10)  \n",
    "print(recommendations_advanced1[['movieId', 'title', 'combined_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId                                              title  \\\n",
      "1100     1197                                 The Princess Bride   \n",
      "5788     6232                                          Born Free   \n",
      "10217   54259                                           Stardust   \n",
      "1962     2161                              The NeverEnding Story   \n",
      "2865     3105                                         Awakenings   \n",
      "9304    41566  The Chronicles of Narnia: The Lion, the Witch ...   \n",
      "12067   79318                                      Winnebago Man   \n",
      "3901     4210                                          Manhunter   \n",
      "7422     8633                               The Last Starfighter   \n",
      "4768     5126                                  The Deadly Mantis   \n",
      "\n",
      "       combined_score  \n",
      "1100         0.394854  \n",
      "5788         0.308018  \n",
      "10217        0.303716  \n",
      "1962         0.300134  \n",
      "2865         0.278083  \n",
      "9304         0.267593  \n",
      "12067        0.250902  \n",
      "3901         0.249790  \n",
      "7422         0.243689  \n",
      "4768         0.238940  \n"
     ]
    }
   ],
   "source": [
    "recommendations_advanced2 = recommender_advanced.recommend('2', top_n=10)  \n",
    "print(recommendations_advanced2[['movieId', 'title', 'combined_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advanced model extends the baseline content recommender by including additional movie attributes—weighted_score, sentiment_polarity, and movie_age—as well as feature scaling for weighted_score and movie_age. This evolution enables the model to consider not only thematic content similarity, but also movie quality, viewer sentiment, and timeliness. By combining these various factors, the advanced model hopes to provide more nuanced and personalized recommendations. The strategic inclusion and scaling of these features improves the model's ability to better align recommendations with individual user preferences, potentially improving recommendation accuracy and user satisfaction over the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation of content-based models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the sampling technique used is to calculate a statistically significant sample size in order to estimate the proportion of movies rated 4.0 or higher in a dataset. This decision is based on a specific confidence level (95%) and margin of error (5%), with the goal of obtaining precise and reliable inferences about the population's characteristics from a sample of data. The method used employs a standard formula that includes the Z-score associated with the desired confidence level and the estimated proportion of interest, ensuring that the sample size is sufficient to accurately reflect the population. This technique is critical for designing studies or analyses that require accurate estimations of population parameters for decision-making or hypothesis testing, as it minimizes potential biases and errors caused by small or arbitrarily chosen sample sizes. By rigorously determining the required sample size, the approach improves the credibility and validity of the findings derived from the sample data, making it a cornerstone of statistical analysis and research methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size: 385\n"
     ]
    }
   ],
   "source": [
    "def calculate_sample_size(confidence_level, margin_of_error, proportion):\n",
    "    z_score = abs(scipy.stats.norm.ppf((1 - confidence_level) / 2))\n",
    "    sample_size = math.ceil((z_score ** 2 * proportion * (1 - proportion)) / (margin_of_error ** 2))\n",
    "    return sample_size\n",
    "\n",
    "confidence_level = 0.95\n",
    "margin_of_error = 0.05\n",
    "\n",
    "proportion_higher_ratings = df_ratings[df_ratings['rating'] >= 4.0].shape[0] / df_ratings.shape[0]\n",
    "required_sample_size = calculate_sample_size(confidence_level, margin_of_error, proportion_higher_ratings)\n",
    "print(f\"Required sample size: {required_sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_movie_ids = np.random.choice(df_grouped['movieId'].unique(), size=required_sample_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df_ratings = df_ratings[df_ratings['movieId'].isin(sample_movie_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_movie(movie_id, df_ratings, recommender, top_n=10):\n",
    "    \"\"\"Evaluate a single movie for the recommender system, adjusted for actual user ratings.\"\"\"\n",
    "    recommendations = recommender.recommend(str(movie_id), top_n=top_n)\n",
    "    if recommendations.empty:\n",
    "        return np.array([]), None \n",
    "\n",
    "    recommended_ids = recommendations['movieId'].astype(str).tolist()\n",
    "    \n",
    "    matching_ratings = df_ratings[df_ratings['movieId'].astype(str).isin(recommended_ids)]\n",
    "    \n",
    "    hit_rate = (matching_ratings['rating'] >= 4.0).mean() if not matching_ratings.empty else None\n",
    "\n",
    "    return np.array(matching_ratings['rating']), hit_rate\n",
    "\n",
    "def evaluate_recommender(df_ratings, recommender, sample_movie_ids, top_n=10, threshold=4.0):\n",
    "    \"\"\"Evaluate the recommender system using sampled movie IDs, including adjusted hit rate.\"\"\"\n",
    "    all_ratings, hit_rates = [], []\n",
    "\n",
    "    for movie_id in sample_movie_ids:\n",
    "        movie_ratings, hit_rate = evaluate_movie(movie_id, df_ratings, recommender, top_n=top_n)\n",
    "        if movie_ratings.size > 0:\n",
    "            all_ratings.extend(movie_ratings)\n",
    "        if hit_rate is not None:\n",
    "            hit_rates.append(hit_rate)\n",
    "    \n",
    "    all_ratings = np.array(all_ratings)\n",
    "    if len(all_ratings) > 0:\n",
    "        mae = np.mean(np.abs(all_ratings - 5))\n",
    "        mse = np.mean((all_ratings - 5) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        precision = np.sum(all_ratings >= threshold) / len(all_ratings)\n",
    "    else:\n",
    "        mae, mse, rmse, precision = 0, 0, 0, 0\n",
    "\n",
    "    avg_hit_rate = np.mean(hit_rates) if hit_rates else None  \n",
    "\n",
    "    print(f\"Sample Size: {len(sample_movie_ids)}\")\n",
    "    print(f\"MAE: {mae:.4f}\\nMSE: {mse:.4f}\\nRMSE: {rmse:.4f}\\nPrecision: {precision:.4f}\\nAverage Hit Rate: {avg_hit_rate if avg_hit_rate is not None else 'N/A'}\")\n",
    "\n",
    "    return mae, mse, rmse, precision, avg_hit_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Baseline-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mae, mse, rmse, precision, avg_hit_rate \u001b[38;5;241m=\u001b[39m evaluate_recommender(df_ratings, recommender_base, sample_movie_ids, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[56], line 20\u001b[0m, in \u001b[0;36mevaluate_recommender\u001b[1;34m(df_ratings, recommender, sample_movie_ids, top_n, threshold)\u001b[0m\n\u001b[0;32m     17\u001b[0m all_ratings, hit_rates \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m movie_id \u001b[38;5;129;01min\u001b[39;00m sample_movie_ids:\n\u001b[1;32m---> 20\u001b[0m     movie_ratings, hit_rate \u001b[38;5;241m=\u001b[39m evaluate_movie(movie_id, df_ratings, recommender, top_n\u001b[38;5;241m=\u001b[39mtop_n)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m movie_ratings\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m         all_ratings\u001b[38;5;241m.\u001b[39mextend(movie_ratings)\n",
      "Cell \u001b[1;32mIn[56], line 9\u001b[0m, in \u001b[0;36mevaluate_movie\u001b[1;34m(movie_id, df_ratings, recommender, top_n)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([]), \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[0;32m      7\u001b[0m recommended_ids \u001b[38;5;241m=\u001b[39m recommendations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m----> 9\u001b[0m matching_ratings \u001b[38;5;241m=\u001b[39m df_ratings[df_ratings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39misin(recommended_ids)]\n\u001b[0;32m     11\u001b[0m hit_rate \u001b[38;5;241m=\u001b[39m (matching_ratings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4.0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matching_ratings\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(matching_ratings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]), hit_rate\n",
      "File \u001b[1;32mc:\\Users\\gilia\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6233\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m   6235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m   6236\u001b[0m     ]\n\u001b[0;32m   6238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6239\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6240\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6243\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gilia\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:448\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, dtype, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "File \u001b[1;32mc:\\Users\\gilia\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Users\\gilia\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    524\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 526\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    528\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    529\u001b[0m newb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[1;32mc:\\Users\\gilia\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gilia\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     values \u001b[38;5;241m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\gilia\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:114\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    111\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mensure_string_array(\n\u001b[0;32m    113\u001b[0m         arr, skipna\u001b[38;5;241m=\u001b[39mskipna, convert_na_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(shape)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_datetime64_dtype(arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint64:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mae, mse, rmse, precision, avg_hit_rate = evaluate_recommender(df_ratings, recommender_base, sample_movie_ids, top_n=10, threshold=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Advanced-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 385\n",
      "MAE: 1.1128\n",
      "MSE: 2.1326\n",
      "RMSE: 1.4603\n",
      "Precision: 0.6535\n",
      "Average Hit Rate: 0.6382666966643713\n"
     ]
    }
   ],
   "source": [
    "mae, mse, rmse, precision, avg_hit_rate = evaluate_recommender(df_ratings, recommender_advanced, sample_movie_ids, top_n=10, threshold=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advanced model outperforms the baseline model on all metrics. It has lower MAE, MSE, and RMSE values, implying that its recommendations are, on average, closer to ideal ratings and less prone to large errors. Its higher precision and average hit rate indicate that it is more effective at recommending movies that users are likely to rate highly (4.0 or higher), demonstrating a better understanding and matching of user preferences. In short, depending on the evaluation method used, the advanced model provides more accurate and user-aligned recommendations than the baseline model, making it the better option for increasing user satisfaction with the recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person.\n",
    "\n",
    "There are two types of collaborative filtering: user-based and item-based. User-based collaborative filtering is based on the similarity between users and item-based collaborative filtering is based on the similarity between items. For our recommender system we chose an item-based approach. The reasons for that are many. Item-based collaborative filtering is often preferred over user-based collaborative filtering, particularly in environments where the item catalog is relatively stable and doesn't grow as quickly as the user base. Item-based systems have a better scalability and efficiency, especially with large user bases. Unlike user preferences, which can change rapidly and complicate similarity calculations, the characteristics of movies remain constant, making it easier to calculate and store the item similarities as their relationship are stable. An item-based approach sidesteps the complexity and computational demand of constantly updating user similarities, making it a more straightforward choice for delivering recommendations also for new users and less popular items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an item-based collaborative filtering system, we need to calculate the similarity between items based on the ratings users have given to those items. We will use the cosine similarity to calculate the similarity between items. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24848104 entries, 0 to 24848103\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   userId            int64         \n",
      " 1   movieId           int64         \n",
      " 2   rating            Float64       \n",
      " 3   timestamp         datetime64[ns]\n",
      " 4   user_mean_rating  Float64       \n",
      " 5   liked_by_user     boolean       \n",
      " 6   weighted_score    float64       \n",
      " 7   average_rating    Float64       \n",
      " 8   rating_count      int64         \n",
      "dtypes: Float64(3), boolean(1), datetime64[ns](1), float64(1), int64(3)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "df_ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.drop(columns=['user_mean_rating', 'liked_by_user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>weighted_score</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.484810e+07</td>\n",
       "      <td>2.484810e+07</td>\n",
       "      <td>24848104.0</td>\n",
       "      <td>2.484810e+07</td>\n",
       "      <td>24848104.0</td>\n",
       "      <td>2.484810e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.350036e+05</td>\n",
       "      <td>1.621173e+04</td>\n",
       "      <td>3.528737</td>\n",
       "      <td>3.488773e+00</td>\n",
       "      <td>3.528737</td>\n",
       "      <td>1.547574e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.817512e+04</td>\n",
       "      <td>3.135802e+04</td>\n",
       "      <td>1.060048</td>\n",
       "      <td>4.132034e-01</td>\n",
       "      <td>0.462782</td>\n",
       "      <td>1.685705e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.776663e+00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.712600e+04</td>\n",
       "      <td>1.088000e+03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.191160e+00</td>\n",
       "      <td>3.253788</td>\n",
       "      <td>3.245000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.351340e+05</td>\n",
       "      <td>2.670000e+03</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.510029e+00</td>\n",
       "      <td>3.60369</td>\n",
       "      <td>9.539000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.026420e+05</td>\n",
       "      <td>6.711000e+03</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.802293e+00</td>\n",
       "      <td>3.87012</td>\n",
       "      <td>2.194600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.708960e+05</td>\n",
       "      <td>1.762750e+05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.411433e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.289500e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId       movieId      rating  weighted_score  average_rating  \\\n",
       "count  2.484810e+07  2.484810e+07  24848104.0    2.484810e+07      24848104.0   \n",
       "mean   1.350036e+05  1.621173e+04    3.528737    3.488773e+00        3.528737   \n",
       "std    7.817512e+04  3.135802e+04    1.060048    4.132034e-01        0.462782   \n",
       "min    1.000000e+00  1.000000e+00         0.5    1.776663e+00             0.5   \n",
       "25%    6.712600e+04  1.088000e+03         3.0    3.191160e+00        3.253788   \n",
       "50%    1.351340e+05  2.670000e+03         3.5    3.510029e+00         3.60369   \n",
       "75%    2.026420e+05  6.711000e+03         4.0    3.802293e+00         3.87012   \n",
       "max    2.708960e+05  1.762750e+05         5.0    4.411433e+00             5.0   \n",
       "\n",
       "       rating_count  \n",
       "count  2.484810e+07  \n",
       "mean   1.547574e+04  \n",
       "std    1.685705e+04  \n",
       "min    1.000000e+00  \n",
       "25%    3.245000e+03  \n",
       "50%    9.539000e+03  \n",
       "75%    2.194600e+04  \n",
       "max    8.289500e+04  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To safe computational time, we will use a subset of the data. We will only use ratings from 2016 onwards. A final implementation could use the entire dataset to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3321925 entries, 1319144 to 16502857\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   userId          int64  \n",
      " 1   movieId         int64  \n",
      " 2   rating          Float64\n",
      " 3   weighted_score  float64\n",
      " 4   average_rating  Float64\n",
      " 5   rating_count    int64  \n",
      "dtypes: Float64(2), float64(1), int64(3)\n",
      "memory usage: 183.7 MB\n"
     ]
    }
   ],
   "source": [
    "#df_ratings['timestamp'] = pd.to_datetime(df_ratings['timestamp'])\n",
    "df_ratings = df_ratings.sort_values('timestamp')\n",
    "df_ratings_subset = df_ratings[df_ratings['timestamp'] > '2016-01-01']\n",
    "df_ratings_subset = df_ratings_subset.drop(columns=['timestamp'])\n",
    "df_ratings_subset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index             13.368927\n",
       "average_rating    13.368927\n",
       "movieId           13.368927\n",
       "rating            13.368927\n",
       "rating_count      13.368927\n",
       "timestamp               NaN\n",
       "userId            13.368927\n",
       "weighted_score    13.368927\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory usage of subset / original\n",
    "(df_ratings_subset.memory_usage() / df_ratings.memory_usage()) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train/test split we will split the data historically. Temporal splitting ensures that the training data contains information from the past, and the test data contains information from the future. This reflects a real-world scenario better, where the system is trained on historical data and evaluated on more recent/future data to assess its performance. We also tried a random split that resulted in better RMSE values than the temporal split. However, we decided to use the temporal split for the sake of a more realistic approach and to align with industry standards. As a model deployment is not possible we can ensure a better real-world performance by that, at this stage.  \n",
    "\n",
    "We will use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Train/Test Split\n",
    "split_index = int(len(df_ratings_subset) * 0.8)\n",
    "\n",
    "train_data = df_ratings_subset[:split_index]\n",
    "test_data = df_ratings_subset[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a similarity matrix. The matrix will contain the similarity between each pair of items. We will use the cosine similarity to calculate the similarity between items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Item Matrix for Training\n",
    "user_item_matrix_train = train_data.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Item-Item Similarity Matrix\n",
    "item_similarity = cosine_similarity(user_item_matrix_train.fillna(0).T)\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix_train.columns, columns=user_item_matrix_train.columns)\n",
    "\n",
    "print(item_similarity_df)\n",
    "print(item_similarity_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our output we encounter a first problem: the matrix size. By only using roughly 13 % of the original data (calculated in memory usage) we end up with a similarity matrix of almost 10 GB in size. This is not feasible for our use case. Consequently, we will implement a Singular Value Decomposition (SVD) to reduce the dimensionality of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SVD helps in extracting latent factors that explain observed ratings, efficiently reducing data dimensionality while preserving essential information. This significantly speeds up calculations, making the process of predicting ratings more efficient, especially when dealing with a large dataset like ours. Additionally, by focusing on these latent factors, SVD enables a deeper understanding of user preferences and item characteristics, promising more personalized and accurate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix\n",
    "user_item_matrix_train = train_data.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "user_item_matrix_sparse = csr_matrix(user_item_matrix_train.values.astype(float))\n",
    "\n",
    "# mean centering\n",
    "mean_user_rating = user_item_matrix_sparse.mean(axis=1)\n",
    "user_item_matrix_centered = user_item_matrix_sparse - mean_user_rating\n",
    "\n",
    "# SVD \n",
    "U, sigma, Vt = svds(user_item_matrix_centered, k=50) # k selected manually at this stage\n",
    "sigma_matrix = np.diag(sigma)\n",
    "\n",
    "# Predict ratings for all users\n",
    "all_user_predicted_ratings = np.dot(np.dot(U, sigma_matrix), Vt) + mean_user_rating.A1.reshape(-1, 1)\n",
    "\n",
    "# Create a DataFrame with the predicted ratings\n",
    "preds_df = pd.DataFrame(all_user_predicted_ratings, index=user_item_matrix_train.index, columns=user_item_matrix_train.columns)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "def safe_get_prediction(row):\n",
    "    try:\n",
    "        return preds_df.loc[row['userId'], row['movieId']]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "test_data['predicted'] = test_data.apply(safe_get_prediction, axis=1)\n",
    "\n",
    "# filter only rows where we have a prediction\n",
    "filtered_test_data = test_data.dropna(subset=['predicted'])\n",
    "\n",
    "# RMSE \n",
    "rmse = sqrt(mean_squared_error(filtered_test_data['rating'], filtered_test_data['predicted']))\n",
    "print(f'RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 1-to-5 scale, an RMSE of 3.165 is quite high, indicating that the predictions can be quite far off from the actual ratings. Let's try to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we will use the surprise library. Surprise automatically handles normalization and scaling of the data as well as the handling of cold start and sparsity issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9694\n",
      "0.9693988323484404\n",
      "MAE:  0.7421\n",
      "0.7421251681528029\n"
     ]
    }
   ],
   "source": [
    "reader = Reader()\n",
    "data = Dataset.load_from_df(train_data[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "svd = SVD()\n",
    "\n",
    "# Fit the model \n",
    "svd.fit(data.build_full_trainset())\n",
    "\n",
    "# Predict ratings for the test set\n",
    "testset = list(zip(test_data['userId'].values, test_data['movieId'].values, test_data['rating'].values))\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "print(accuracy.rmse(predictions))\n",
    "print(accuracy.mae(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a major improvement of the metrics when using the Surprise library compared to our previous approach!\n",
    "\n",
    "A Root Mean Square Error (RMSE) of approximately 0.9694 suggests that, on average, our predicted ratings deviate from the actual ratings by around 0.97 units on a scale of 1 to 5. Without considering their direction, they deviate around around 0.7420 units (MAE). We consider this level of error as moderate to good. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the RMSE and MAE with a random split for illustrative purposes before fine tuning the model on a temporal split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.7848  0.7854  0.7840  0.7847  0.7846  0.7847  0.0005  \n",
      "MAE (testset)     0.5841  0.5849  0.5834  0.5838  0.5838  0.5840  0.0005  \n",
      "Fit time          18.08   18.23   18.77   18.17   18.26   18.30   0.24    \n",
      "Test time         2.99    4.10    2.73    2.45    2.66    2.99    0.58    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.78475205, 0.78539078, 0.78395027, 0.78474373, 0.78458848]),\n",
       " 'test_mae': array([0.58412186, 0.58493104, 0.58343147, 0.58379567, 0.58384229]),\n",
       " 'fit_time': (18.07552695274353,\n",
       "  18.23200798034668,\n",
       "  18.76857304573059,\n",
       "  18.16711926460266,\n",
       "  18.25535297393799),\n",
       " 'test_time': (2.991689920425415,\n",
       "  4.100869178771973,\n",
       "  2.7325620651245117,\n",
       "  2.454019784927368,\n",
       "  2.6558640003204346)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader_random = Reader()\n",
    "\n",
    "data_random_split = Dataset.load_from_df(df_ratings_subset[['userId', 'movieId', 'rating']], reader_random)\n",
    "\n",
    "svd_random = SVD()\n",
    "\n",
    "cross_validate(svd_random, data_random_split, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The superior performance of the random split (model RMSE 0.7847) suggests that it may offer a more balanced and varied dataset for both training and testing phases, potentially leading to a model that is better at generalizing across the entire dataset. \n",
    "\n",
    "Yet, as already mentioned for a real-world recommender systems, a temporal split is often preferred to account for evolving preferences and trends over time. For a movie recommender system, especially one like DreamStream that might experience frequent updates to its movie catalog and shifts in user preferences, we  suggest a temporal split. This approach acknowledges the evolving nature of both movies and user tastes, preparing the system to adapt to real-world scenarios more effectively. It also allows the system to better handle cold start problems with new releases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to our temporal split and try to optimize our model using a GridSearch to find the best combination of hyperparameter for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9689\n",
      "RMSE: 0.9687\n",
      "RMSE: 0.9690\n",
      "RMSE: 0.9686\n",
      "RMSE: 0.9723\n",
      "RMSE: 0.9703\n",
      "RMSE: 0.9720\n",
      "RMSE: 0.9699\n",
      "Best RMSE score obtained:  0.968631935393709\n",
      "Best parameters:  {'lr_all': 0.005, 'n_epochs': 20, 'n_factors': 100, 'reg_all': 0.05}\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "data = Dataset.load_from_df(train_data[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "testset = list(zip(test_data['userId'].values, test_data['movieId'].values, test_data['rating'].values))\n",
    "\n",
    "# our grid of parameters\n",
    "param_grid = {'n_factors': [50, 100],  # Number of factors\n",
    "              'n_epochs': [20],         # Number of iterations\n",
    "              'lr_all': [0.005, 0.01],      # Learning rate\n",
    "              'reg_all': [0.02, 0.05]}      # Regularization term\n",
    "\n",
    "svd = SVD()\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Loop through parameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    svd = SVD(**params)\n",
    "    svd.fit(trainset)\n",
    "\n",
    "\n",
    "    predictions = svd.test(testset)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "\n",
    "    # Update best RMSE and parameters if necessary\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best RMSE score obtained: \", best_rmse)\n",
    "print(\"Best parameters: \", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best RMSE score obtained is 0.9686 with the following parameters: \n",
    "\n",
    "lr_all: 0.005, n_epochs: 20, n_factors: 100, reg_all: 0.05\n",
    "\n",
    "This is a slightly  better RMSE score as we obtained with the default parameters (RMSE 0.9698). With higher computational power and time, we could further optimize the model by testing more hyperparameters and combinations. At this stage we will stick with the selected parameters from our GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train the best version of our model on the full subset and predict the top ten recommendations for a selected user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x212dfe3bfd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = SVD(**best_params)\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for user: 14204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommended movies for User 14204:\n",
      "Rank 1: Movie ID 93040, Predicted Rating: 4.195004543536218\n",
      "Rank 2: Movie ID 170705, Predicted Rating: 4.134805189136601\n",
      "Rank 3: Movie ID 140265, Predicted Rating: 4.062797962633411\n",
      "Rank 4: Movie ID 159817, Predicted Rating: 4.054666416352564\n",
      "Rank 5: Movie ID 157373, Predicted Rating: 4.0277859449841396\n",
      "Rank 6: Movie ID 8484, Predicted Rating: 4.020928384373775\n",
      "Rank 7: Movie ID 82143, Predicted Rating: 4.002642547273006\n",
      "Rank 8: Movie ID 5475, Predicted Rating: 3.984403508927792\n",
      "Rank 9: Movie ID 111130, Predicted Rating: 3.9779020069275632\n",
      "Rank 10: Movie ID 107412, Predicted Rating: 3.977519160419064\n"
     ]
    }
   ],
   "source": [
    "selected_user_id = 14204\n",
    "rated_movie_ids = df_ratings_subset[df_ratings_subset['userId'] == selected_user_id]['movieId'].unique()\n",
    "all_movie_ids = df_ratings_subset['movieId'].unique()\n",
    "\n",
    "# Predict ratings for all movies that the selected user has not rated yet\n",
    "predicted_unrated_movies = []\n",
    "for movie_id in all_movie_ids:\n",
    "    if movie_id not in rated_movie_ids:\n",
    "        prediction = svd.predict(uid=selected_user_id, iid=movie_id)\n",
    "        predicted_unrated_movies.append((movie_id, prediction.est))\n",
    "\n",
    "# sorting\n",
    "sorted_predicted_unrated_movies = sorted(predicted_unrated_movies, key=lambda x: x[1], reverse=True)\n",
    "top_10_unrated_movies = sorted_predicted_unrated_movies[:10]\n",
    "\n",
    "# Top 10 predicted ratings for the selected user\n",
    "print(f\"Top 10 recommended movies for User {selected_user_id}:\")\n",
    "for i, (movie_id, predicted_rating) in enumerate(top_10_unrated_movies, start=1):\n",
    "    print(f\"Rank {i}: Movie ID {movie_id}, Predicted Rating: {predicted_rating}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our pursuit of creating a more nuanced and effective recommendation system, we've decided to integrate our two models — a content-based model and an item-based collaborative filtering model using SVD — into a singular hybrid approach. This approach is driven by our goal to combine the strengths of both models: the content-based model's ability to recommend items based on their intrinsic properties and similarities, and the collaborative filtering model's capacity to incorporate user preferences and historical interactions to predict item ratings with high accuracy. By combining these approaches, we aim to deliver more personalized, diverse, and contextually relevant recommendations, thereby enhancing user satisfaction and engagement with our platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_model(movie_id, user_id, top_n, svd_model):\n",
    "    # Step 1: Get top N content-based recommendations\n",
    "    content_recs = recommender_advanced.recommend(movie_id, top_n)\n",
    "\n",
    "    content_recs['movieId'] = content_recs['movieId'].astype(int)\n",
    "\n",
    "    # Step 2: Apply SVD to predict ratings for the top N movies\n",
    "    content_recs['predicted_rating'] = content_recs['movieId'].apply(\n",
    "        lambda x: svd_model.predict(user_id, x).est\n",
    "    )\n",
    "    \n",
    "    # Step 3: Sort recommendations by predicted ratings, then by cosine similarity\n",
    "    final_recs = content_recs.sort_values(\n",
    "        by=['predicted_rating', 'combined_score'], ascending=[False, False]\n",
    "    )\n",
    "    \n",
    "    final_recs = final_recs[['movieId', 'title', 'combined_score', 'predicted_rating']]\n",
    "\n",
    "    return final_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>combined_score</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>1244</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>0.315088</td>\n",
       "      <td>3.948005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1103</td>\n",
       "      <td>Rebel Without a Cause</td>\n",
       "      <td>0.302987</td>\n",
       "      <td>3.775396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>1230</td>\n",
       "      <td>Annie Hall</td>\n",
       "      <td>0.343074</td>\n",
       "      <td>3.762786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12007</th>\n",
       "      <td>78499</td>\n",
       "      <td>Toy Story 3</td>\n",
       "      <td>0.378643</td>\n",
       "      <td>3.677054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4339</td>\n",
       "      <td>Von Ryan's Express</td>\n",
       "      <td>0.314689</td>\n",
       "      <td>3.664717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>3114</td>\n",
       "      <td>Toy Story 2</td>\n",
       "      <td>0.515295</td>\n",
       "      <td>3.618099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>5974</td>\n",
       "      <td>The Thief of Bagdad</td>\n",
       "      <td>0.355765</td>\n",
       "      <td>3.578651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7112</th>\n",
       "      <td>7987</td>\n",
       "      <td>Dolls</td>\n",
       "      <td>0.323468</td>\n",
       "      <td>3.460829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>596</td>\n",
       "      <td>Pinocchio</td>\n",
       "      <td>0.308571</td>\n",
       "      <td>3.387574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>2355</td>\n",
       "      <td>A Bug's Life</td>\n",
       "      <td>0.313347</td>\n",
       "      <td>3.352371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                  title  combined_score  predicted_rating\n",
       "1143      1244              Manhattan        0.315088          3.948005\n",
       "1024      1103  Rebel Without a Cause        0.302987          3.775396\n",
       "1130      1230             Annie Hall        0.343074          3.762786\n",
       "12007    78499            Toy Story 3        0.378643          3.677054\n",
       "4019      4339     Von Ryan's Express        0.314689          3.664717\n",
       "2874      3114            Toy Story 2        0.515295          3.618099\n",
       "5555      5974    The Thief of Bagdad        0.355765          3.578651\n",
       "7112      7987                  Dolls        0.323468          3.460829\n",
       "576        596              Pinocchio        0.308571          3.387574\n",
       "2142      2355           A Bug's Life        0.313347          3.352371"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model(1, 2, 10, svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>combined_score</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10576</th>\n",
       "      <td>58559</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>0.257266</td>\n",
       "      <td>4.018121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>428</td>\n",
       "      <td>A Bronx Tale</td>\n",
       "      <td>0.249364</td>\n",
       "      <td>3.905122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>3039</td>\n",
       "      <td>Trading Places</td>\n",
       "      <td>0.236398</td>\n",
       "      <td>3.776068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5610</th>\n",
       "      <td>6035</td>\n",
       "      <td>Pépé le Moko</td>\n",
       "      <td>0.294232</td>\n",
       "      <td>3.775973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6543</th>\n",
       "      <td>7076</td>\n",
       "      <td>Bullitt</td>\n",
       "      <td>0.251069</td>\n",
       "      <td>3.767918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10367</th>\n",
       "      <td>55765</td>\n",
       "      <td>American Gangster</td>\n",
       "      <td>0.236905</td>\n",
       "      <td>3.720463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>26774</td>\n",
       "      <td>Innocent Blood</td>\n",
       "      <td>0.318950</td>\n",
       "      <td>3.669139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>322</td>\n",
       "      <td>Swimming with Sharks</td>\n",
       "      <td>0.265640</td>\n",
       "      <td>3.541532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>5487</td>\n",
       "      <td>Harry and Walter Go To New York</td>\n",
       "      <td>0.237823</td>\n",
       "      <td>3.471251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>30818</td>\n",
       "      <td>Beyond the Sea</td>\n",
       "      <td>0.237414</td>\n",
       "      <td>3.339717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                            title  combined_score  \\\n",
       "10576    58559                  The Dark Knight        0.257266   \n",
       "411        428                     A Bronx Tale        0.249364   \n",
       "2800      3039                   Trading Places        0.236398   \n",
       "5610      6035                     Pépé le Moko        0.294232   \n",
       "6543      7076                          Bullitt        0.251069   \n",
       "10367    55765                American Gangster        0.236905   \n",
       "8230     26774                   Innocent Blood        0.318950   \n",
       "310        322             Swimming with Sharks        0.265640   \n",
       "5110      5487  Harry and Walter Go To New York        0.237823   \n",
       "8590     30818                   Beyond the Sea        0.237414   \n",
       "\n",
       "       predicted_rating  \n",
       "10576          4.018121  \n",
       "411            3.905122  \n",
       "2800           3.776068  \n",
       "5610           3.775973  \n",
       "6543           3.767918  \n",
       "10367          3.720463  \n",
       "8230           3.669139  \n",
       "310            3.541532  \n",
       "5110           3.471251  \n",
       "8590           3.339717  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model(50, 2, 10, svd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
