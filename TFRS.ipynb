{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {
    "id": "goQJe1d1i8ZU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we will use the Tensorflow Recommenders (TFRS) library to build a recommendation system. TFRS is a library for building deep learning recommendation systems. It helps with the full workflow of building a recommendation system: data preparation, model formulation, training, evaluation, and deployment. TFRS is built on top of TensorFlow 2 and Keras, and is designed to be scalable and easy to use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are first focusing on a retrieval system, which is a model that predicts a set of movies from the catalogue that the user is likely to watch. We're going to treat the dataset as an implicit system. This means that we are not trying to predict the rating that a user will give to a movie. Instead, we are trying to rank movies by their relevance to the user. This is a common scenario in many recommendation systems, where we are trying to predict the items that a user is most likely to interact with.\n",
    "Treating Movielens as an implicit system means that we're interpreting users' actions (watching movies) as indicators of their preferences. Specifically:\n",
    "\n",
    "1. Every movie a user has watched is considered a positive example, indicating that they like or are interested in that movie.\n",
    "2. Every movie a user hasn't watched is treated as an implicit negative example, implying that they haven't shown interest in it or haven't been exposed to it yet.\n",
    "\n",
    "This approach helps us make predictions about which movies users might enjoy based on their past behavior without requiring explicit feedback or ratings for each movie."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a second step we will build a ranking model that predicts the top 10 movies that a user is likely to watch. This model will be trained on the same dataset as the retrieval model, but will be optimized for ranking accuracy rather than relevance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSN-y-Lxi6QD",
    "outputId": "ff8cb40e-fdf6-4b1f-d391-747e4bcf26b1",
    "ExecuteTime": {
     "end_time": "2024-04-29T16:51:03.957958Z",
     "start_time": "2024-04-29T16:50:33.327918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not find a version that satisfies the requirement scann (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for scann\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ],
   "metadata": {
    "id": "IWgpA23Mi_et",
    "ExecuteTime": {
     "end_time": "2024-04-29T16:52:19.082987Z",
     "start_time": "2024-04-29T16:51:57.024783Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "id": "tqd1icB9jewq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the TFRS we will use the movielens dataset from [Tensorflow](https://www.tensorflow.org/datasets/catalog/movielens) which is specifically designed to work with TFRS. It is equal to the dataset that was used for the Preprocessing and EDA. However, we will use a subset dataset that is able to run.\n",
    "\n",
    "- 100k-ratings: This dataset contains 100,000 ratings from 943 users on 1,682 movies. This dataset is the oldest version of the MovieLens dataset.\n",
    "- 100k-movies: This dataset contains data of 1,682 movies rated in the 100k dataset."
   ],
   "metadata": {
    "id": "v31WZw6uqI1M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "\n",
    "#Movie Data\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ],
   "metadata": {
    "id": "JfmM4T97jikQ",
    "ExecuteTime": {
     "end_time": "2024-04-29T16:53:06.614526Z",
     "start_time": "2024-04-29T16:52:49.363905Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:52:49.471188: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 32.41 MiB, total: 37.10 MiB) to /Users/maltehaupt/tensorflow_datasets/movielens/100k-ratings/0.1.1...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4000698108324a42a2409b477f7a7de5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca7925a566854af78e7d64c96c248277"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cff170ba47a6436cb5165f442d8cfe4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ee97f961e374dc8b04aa31cb7c309e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train examples...:   0%|          | 0/100000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ee65f1b730e4857a57b8203b5ddbc80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /Users/maltehaupt/tensorflow_datasets/movielens/100k-ratings/0.1.1.incompleteUYI41Y/movielens-train.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "614d8a502c9e4542a96782a87cf99fb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset movielens downloaded and prepared to /Users/maltehaupt/tensorflow_datasets/movielens/100k-ratings/0.1.1. Subsequent calls will reuse this data.\u001B[0m\n",
      "\u001B[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 150.35 KiB, total: 4.84 MiB) to /Users/maltehaupt/tensorflow_datasets/movielens/100k-movies/0.1.1...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c146f5aa8df84b16a2cb7d15bdd4f3b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ddab062a0ee4bd7b10574b56f149dd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9625a6d0a33457fbb0da2a44d5be19f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e29ec34860e640848e3c855d746940ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train examples...:   0%|          | 0/1682 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "619113849063425d84f5777a76616fcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /Users/maltehaupt/tensorflow_datasets/movielens/100k-movies/0.1.1.incompleteJ24QH0/movielens-train.t…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4b652745c814b01a1283537e40afc9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset movielens downloaded and prepared to /Users/maltehaupt/tensorflow_datasets/movielens/100k-movies/0.1.1. Subsequent calls will reuse this data.\u001B[0m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNvbVP6MlMyg",
    "outputId": "85671955-09a3-452e-b0b7-a7344872156d",
    "ExecuteTime": {
     "end_time": "2024-04-29T16:53:06.698467Z",
     "start_time": "2024-04-29T16:53:06.617611Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:53:06.694864: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-04-29 17:53:06.696368: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOn6WvEDlO-P",
    "outputId": "b1a58507-cd61-42d8-de00-09c17f857bd0",
    "ExecuteTime": {
     "end_time": "2024-04-29T16:53:06.717216Z",
     "start_time": "2024-04-29T16:53:06.699989Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 17:53:06.715090: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-04-29 17:53:06.715368: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this Notebook we only going to focus on the ratings dataset. We keep only the user_id, and movie_title fields in the dataset."
   ],
   "metadata": {
    "id": "6syOWPSqk7Zo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ],
   "metadata": {
    "id": "4t0gUua7k1tq",
    "ExecuteTime": {
     "end_time": "2024-04-29T16:53:06.740719Z",
     "start_time": "2024-04-29T16:53:06.718040Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Shuffle the dataset with a specified buffer size and seed\n",
    "shuffled = ratings.shuffle(buffer_size=100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "# Take the first 80,000 examples for the training set (80% of the data)\n",
    "train_dataset = shuffled.take(80_000)\n",
    "\n",
    "# Skip the first 80,000 examples and take the next 20,000 for the test set (20% of the data)\n",
    "test_dataset = shuffled.skip(80_000).take(20_000)"
   ],
   "metadata": {
    "id": "7vr1R4oVvFZb",
    "ExecuteTime": {
     "end_time": "2024-04-29T16:53:09.624672Z",
     "start_time": "2024-04-29T16:53:09.599446Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets investigate the unique user ids and movie titles in the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 18:04:10.996154: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-04-29 18:04:12.294915: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n       b'2 Days in the Valley (1996)',\n       b'20,000 Leagues Under the Sea (1954)',\n       b'2001: A Space Odyssey (1968)',\n       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n       b'39 Steps, The (1935)'], dtype=object)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique user ids and movie titles\n",
    "user_ids_vocabulary = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "movie_titles_vocabulary = ratings.batch(1_000_000).map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles_vocabulary)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids_vocabulary)))\n",
    "\n",
    "unique_movie_titles[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T17:04:12.404957Z",
     "start_time": "2024-04-29T17:04:08.894319Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The extraction and identification of unique user IDs and movie titles from a dataset are crucial for efficiently managing and processing data in machine learning applications, especially in recommender systems. By converting these categorical labels into unique numerical identifiers, we can optimize memory usage and enhance computational efficiency, which is fundamental for training accurate and scalable models. This preprocessing step also ensures that each entity (user or movie) is uniquely represented, aiding in the precise construction of user-item interaction matrices that are essential for personalization and prediction accuracy in recommendation algorithms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will build a two-tower retrieval model using the TFRS library. This model consists of two separate components or \"towers\".\n",
    "\n",
    "A two-tower retrieval model is a type of architecture commonly used in recommendation systems, particularly in the context of information retrieval tasks such as recommending items to users. The \"two towers\" refer to two separate components or \"towers\" within the model:\n",
    "\n",
    "Query Tower: This tower represents the user's query or input. It typically takes user-specific information or features, such as user demographics, past interactions, or explicit preferences, and encodes them into a fixed-length representation.\n",
    "Candidate Tower: This tower represents the items in the catalog or database that are being considered for recommendation. It takes item-specific information or features, such as item attributes, metadata, or embeddings, and encodes them into a fixed-length representation.\n",
    "After encoding both the query and candidate items, the model computes a similarity score between them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
