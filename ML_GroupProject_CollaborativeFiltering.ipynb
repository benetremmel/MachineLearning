{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:15.572546Z",
     "start_time": "2024-04-02T15:02:14.594443Z"
    }
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install surprise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11a3b27dcab4d935"
  },
  {
   "cell_type": "markdown",
   "id": "9f3f1ed81e5340dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622d2a2241596f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person.\n",
    "\n",
    "There are two types of collaborative filtering: user-based and item-based. User-based collaborative filtering is based on the similarity between users and item-based collaborative filtering is based on the similarity between items. For our recommender system we chose an item-based approach. The reasons for that are many. Item-based collaborative filtering is often preferred over user-based collaborative filtering, particularly in environments where the item catalog is relatively stable and doesn't grow as quickly as the user base. Item-based systems have a better scalability and efficiency, especially with large user bases. Unlike user preferences, which can change rapidly and complicate similarity calculations, the characteristics of movies remain constant, making it easier to calculate and store the item similarities as their relationship are stable. An item-based approach sidesteps the complexity and computational demand of constantly updating user similarities, making it a more straightforward choice for delivering recommendations also for new users and less popular items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8fea516d50400",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21248191d2e61fc8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To build an item-based collaborative filtering system, we need to calculate the similarity between items based on the ratings users have given to those items. We will use the cosine similarity to calculate the similarity between items. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bf0a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33512e045660dbaf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:20.027235Z",
     "start_time": "2024-04-02T15:02:19.602600Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged = pd.read_pickle('data/df_movies_cleaned.pkl')\n",
    "df_ratings = pd.read_pickle('data/df_ratings_cleaned.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a37f4ffff215770",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:20.923974Z",
     "start_time": "2024-04-02T15:02:20.916794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24848104 entries, 0 to 24848103\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   userId            int64         \n",
      " 1   movieId           int64         \n",
      " 2   rating            Float64       \n",
      " 3   timestamp         datetime64[ns]\n",
      " 4   user_mean_rating  Float64       \n",
      " 5   liked_by_user     boolean       \n",
      "dtypes: Float64(2), boolean(1), datetime64[ns](1), int64(2)\n",
      "memory usage: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "df_ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.drop(columns=['user_mean_rating', 'liked_by_user'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:22.578823Z",
     "start_time": "2024-04-02T15:02:22.479280Z"
    }
   },
   "id": "b18e922010ffc01c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4851b0a6f34b61d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:24.491386Z",
     "start_time": "2024-04-02T15:02:23.243393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             userId       movieId      rating                      timestamp\ncount  2.484810e+07  2.484810e+07  24848104.0                       24848104\nmean   1.350036e+05  1.621173e+04    3.528737  2007-02-21 23:13:44.095162112\nmin    1.000000e+00  1.000000e+00         0.5            1995-01-09 11:46:44\n25%    6.712600e+04  1.088000e+03         3.0            2001-06-09 05:22:35\n50%    1.351340e+05  2.670000e+03         3.5     2006-06-16 19:53:25.500000\n75%    2.026420e+05  6.711000e+03         4.0  2013-02-19 17:38:24.249999872\nmax    2.708960e+05  1.762750e+05         5.0            2017-08-04 06:57:50\nstd    7.817512e+04  3.135802e+04    1.060048                            NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.484810e+07</td>\n      <td>2.484810e+07</td>\n      <td>24848104.0</td>\n      <td>24848104</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.350036e+05</td>\n      <td>1.621173e+04</td>\n      <td>3.528737</td>\n      <td>2007-02-21 23:13:44.095162112</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.5</td>\n      <td>1995-01-09 11:46:44</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.712600e+04</td>\n      <td>1.088000e+03</td>\n      <td>3.0</td>\n      <td>2001-06-09 05:22:35</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.351340e+05</td>\n      <td>2.670000e+03</td>\n      <td>3.5</td>\n      <td>2006-06-16 19:53:25.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.026420e+05</td>\n      <td>6.711000e+03</td>\n      <td>4.0</td>\n      <td>2013-02-19 17:38:24.249999872</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.708960e+05</td>\n      <td>1.762750e+05</td>\n      <td>5.0</td>\n      <td>2017-08-04 06:57:50</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.817512e+04</td>\n      <td>3.135802e+04</td>\n      <td>1.060048</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993db599e3f72317",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To safe computational time, we will use a subset of the data. We will only use ratings from 2016 onwards. A final implementation could use the entire dataset to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f2debee7e5e141",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:26.664800Z",
     "start_time": "2024-04-02T15:02:24.492284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3321925 entries, 1319144 to 16502857\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   userId   int64  \n",
      " 1   movieId  int64  \n",
      " 2   rating   Float64\n",
      "dtypes: Float64(1), int64(2)\n",
      "memory usage: 104.5 MB\n"
     ]
    }
   ],
   "source": [
    "#df_ratings['timestamp'] = pd.to_datetime(df_ratings['timestamp'])\n",
    "df_ratings = df_ratings.sort_values('timestamp')\n",
    "df_ratings_subset = df_ratings[df_ratings['timestamp'] > '2016-01-01']\n",
    "df_ratings_subset = df_ratings_subset.drop(columns=['timestamp'])\n",
    "df_ratings_subset.info()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Index        13.368927\nmovieId      13.368927\nrating       13.368927\ntimestamp          NaN\nuserId       13.368927\ndtype: float64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory usage of subset / original\n",
    "(df_ratings_subset.memory_usage() / df_ratings.memory_usage()) * 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:26.668504Z",
     "start_time": "2024-04-02T15:02:26.665670Z"
    }
   },
   "id": "a09969fe103f06",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the train/test split we will split the data historically. Temporal splitting ensures that the training data contains information from the past, and the test data contains information from the future. This reflects a real-world scenario better, where the system is trained on historical data and evaluated on more recent/future data to assess its performance. We also tried a random split that resulted in better RMSE values than the temporal split. However, we decided to use the temporal split for the sake of a more realistic approach and to align with industry standards. As a model deployment is not possible we can ensure a better real-world performance by that, at this stage.  \n",
    "\n",
    "We will use 80% of the data for training and 20% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44840797c8623209"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Temporal Train/Test Split\n",
    "split_index = int(len(df_ratings_subset) * 0.8)\n",
    "\n",
    "train_data = df_ratings_subset[:split_index]\n",
    "test_data = df_ratings_subset[split_index:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:02:26.670971Z",
     "start_time": "2024-04-02T15:02:26.669201Z"
    }
   },
   "id": "1ee56b1b9b2ce50d",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now create a similarity matrix. The matrix will contain the similarity between each pair of items. We will use the cosine similarity to calculate the similarity between items."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8cfbda882a737d2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974bf9e6816ac6d2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:05:21.800337Z",
     "start_time": "2024-04-02T15:02:27.349223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId    1         2         3         4         5         6         7       \\\n",
      "movieId                                                                         \n",
      "1        1.000000  0.379882  0.132247  0.033639  0.164564  0.237477  0.122782   \n",
      "2        0.379882  1.000000  0.177338  0.053671  0.175422  0.191025  0.113127   \n",
      "3        0.132247  0.177338  1.000000  0.048691  0.283075  0.120667  0.173906   \n",
      "4        0.033639  0.053671  0.048691  1.000000  0.114062  0.031977  0.036082   \n",
      "5        0.164564  0.175422  0.283075  0.114062  1.000000  0.109077  0.268335   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "170747   0.010918  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "170749   0.000000  0.000000  0.064965  0.000000  0.000000  0.000000  0.000000   \n",
      "170751   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "170753   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "170755   0.000000  0.020036  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "movieId    8         9         10      ...  170735  170739    170741  \\\n",
      "movieId                                ...                             \n",
      "1        0.058917  0.077743  0.263137  ...     0.0     0.0  0.018196   \n",
      "2        0.111996  0.121150  0.264318  ...     0.0     0.0  0.030054   \n",
      "3        0.111870  0.165357  0.145169  ...     0.0     0.0  0.000000   \n",
      "4        0.066257  0.037098  0.011021  ...     0.0     0.0  0.000000   \n",
      "5        0.097875  0.098231  0.136795  ...     0.0     0.0  0.000000   \n",
      "...           ...       ...       ...  ...     ...     ...       ...   \n",
      "170747   0.000000  0.000000  0.026787  ...     0.0     0.0  0.000000   \n",
      "170749   0.000000  0.000000  0.000000  ...     0.0     0.0  0.000000   \n",
      "170751   0.000000  0.000000  0.000000  ...     0.0     0.0  0.000000   \n",
      "170753   0.000000  0.000000  0.035716  ...     0.0     0.0  0.000000   \n",
      "170755   0.000000  0.000000  0.000000  ...     0.0     0.0  0.000000   \n",
      "\n",
      "movieId    170743    170745    170747    170749  170751  170753    170755  \n",
      "movieId                                                                    \n",
      "1        0.018196  0.000000  0.010918  0.000000     0.0     0.0  0.000000  \n",
      "2        0.030054  0.021252  0.000000  0.000000     0.0     0.0  0.020036  \n",
      "3        0.000000  0.000000  0.000000  0.064965     0.0     0.0  0.000000  \n",
      "4        0.000000  0.000000  0.000000  0.000000     0.0     0.0  0.000000  \n",
      "5        0.000000  0.000000  0.000000  0.000000     0.0     0.0  0.000000  \n",
      "...           ...       ...       ...       ...     ...     ...       ...  \n",
      "170747   0.000000  0.000000  1.000000  0.000000     0.0     0.0  0.000000  \n",
      "170749   0.000000  0.000000  0.000000  1.000000     0.0     0.0  0.000000  \n",
      "170751   0.000000  0.000000  0.000000  0.000000     1.0     0.0  0.000000  \n",
      "170753   0.000000  0.000000  0.000000  0.000000     0.0     1.0  0.000000  \n",
      "170755   0.000000  0.000000  0.000000  0.000000     0.0     0.0  1.000000  \n",
      "\n",
      "[36066 rows x 36066 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36066 entries, 1 to 170755\n",
      "Columns: 36066 entries, 1 to 170755\n",
      "dtypes: float64(36066)\n",
      "memory usage: 9.7 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# User-Item Matrix for Training\n",
    "user_item_matrix_train = train_data.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Item-Item Similarity Matrix\n",
    "item_similarity = cosine_similarity(user_item_matrix_train.fillna(0).T)\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix_train.columns, columns=user_item_matrix_train.columns)\n",
    "\n",
    "print(item_similarity_df)\n",
    "print(item_similarity_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at our output we encounter a first problem: the matrix size. By only using roughly 13 % of the original data (calculated in memory usage) we end up with a similarity matrix of almost 10 GB in size. This is not feasible for our use case. Consequently, we will implement a Singular Value Decomposition (SVD) to reduce the dimensionality of the matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "233b79209f07a069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Singular Value Decomposition (SVD)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e5a78ccfe103a2c"
  },
  {
   "cell_type": "markdown",
   "id": "e408f10ab34450d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "SVD helps in extracting latent factors that explain observed ratings, efficiently reducing data dimensionality while preserving essential information. This significantly speeds up calculations, making the process of predicting ratings more efficient, especially when dealing with a large dataset like ours. Additionally, by focusing on these latent factors, SVD enables a deeper understanding of user preferences and item characteristics, promising more personalized and accurate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.1650502206124087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/d6cx2m1d2nl8b8c4cm_30td00000gn/T/ipykernel_8039/2872111813.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['predicted'] = test_data.apply(safe_get_prediction, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# create matrix\n",
    "user_item_matrix_train = train_data.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "user_item_matrix_sparse = csr_matrix(user_item_matrix_train.values.astype(float))\n",
    "\n",
    "# mean centering\n",
    "mean_user_rating = user_item_matrix_sparse.mean(axis=1)\n",
    "user_item_matrix_centered = user_item_matrix_sparse - mean_user_rating\n",
    "\n",
    "# SVD \n",
    "U, sigma, Vt = svds(user_item_matrix_centered, k=50) # k selected manually at this stage\n",
    "sigma_matrix = np.diag(sigma)\n",
    "\n",
    "# Predict ratings for all users\n",
    "all_user_predicted_ratings = np.dot(np.dot(U, sigma_matrix), Vt) + mean_user_rating.A1.reshape(-1, 1)\n",
    "\n",
    "# Create a DataFrame with the predicted ratings\n",
    "preds_df = pd.DataFrame(all_user_predicted_ratings, index=user_item_matrix_train.index, columns=user_item_matrix_train.columns)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "def safe_get_prediction(row):\n",
    "    try:\n",
    "        return preds_df.loc[row['userId'], row['movieId']]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "test_data['predicted'] = test_data.apply(safe_get_prediction, axis=1)\n",
    "\n",
    "# filter only rows where we have a prediction\n",
    "filtered_test_data = test_data.dropna(subset=['predicted'])\n",
    "\n",
    "# RMSE \n",
    "rmse = sqrt(mean_squared_error(filtered_test_data['rating'], filtered_test_data['predicted']))\n",
    "print(f'RMSE: {rmse}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:11:25.073419Z",
     "start_time": "2024-04-02T15:05:21.801711Z"
    }
   },
   "id": "2914caef28fc5282",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "6b02e169b7e20aa1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given a 1-to-5 scale, an RMSE of 3.165 is quite high, indicating that the predictions can be quite far off from the actual ratings. Let's try to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855480f2",
   "metadata": {},
   "source": [
    "For that, we will use the surprise library. Surprise automatically handles normalization and scaling of the data as well as the handling of cold start and sparsity issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9694\n",
      "0.9693988323484404\n",
      "MAE:  0.7421\n",
      "0.7421251681528029\n"
     ]
    }
   ],
   "source": [
    "reader = Reader()\n",
    "data = Dataset.load_from_df(train_data[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "svd = SVD()\n",
    "\n",
    "# Fit the model \n",
    "svd.fit(data.build_full_trainset())\n",
    "\n",
    "# Predict ratings for the test set\n",
    "testset = list(zip(test_data['userId'].values, test_data['movieId'].values, test_data['rating'].values))\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "print(accuracy.rmse(predictions))\n",
    "print(accuracy.mae(predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:11:45.207106Z",
     "start_time": "2024-04-02T15:11:25.077481Z"
    }
   },
   "id": "f1508ee980eb925c",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see a major improvement of the metrics when using the Surprise library compared to our previous approach!\n",
    "\n",
    "A Root Mean Square Error (RMSE) of approximately 0.9694 suggests that, on average, our predicted ratings deviate from the actual ratings by around 0.97 units on a scale of 1 to 5. Without considering their direction, they deviate around around 0.7420 units (MAE). We consider this level of error as moderate to good. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f88110544ded7e69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also compute the RMSE and MAE with a random split for illustrative purposes before fine tuning the model on a temporal split."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e89416a82918f3a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48577760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:13:53.987941Z",
     "start_time": "2024-04-02T15:11:45.521553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.7848  0.7854  0.7840  0.7847  0.7846  0.7847  0.0005  \n",
      "MAE (testset)     0.5841  0.5849  0.5834  0.5838  0.5838  0.5840  0.0005  \n",
      "Fit time          18.08   18.23   18.77   18.17   18.26   18.30   0.24    \n",
      "Test time         2.99    4.10    2.73    2.45    2.66    2.99    0.58    \n"
     ]
    },
    {
     "data": {
      "text/plain": "{'test_rmse': array([0.78475205, 0.78539078, 0.78395027, 0.78474373, 0.78458848]),\n 'test_mae': array([0.58412186, 0.58493104, 0.58343147, 0.58379567, 0.58384229]),\n 'fit_time': (18.07552695274353,\n  18.23200798034668,\n  18.76857304573059,\n  18.16711926460266,\n  18.25535297393799),\n 'test_time': (2.991689920425415,\n  4.100869178771973,\n  2.7325620651245117,\n  2.454019784927368,\n  2.6558640003204346)}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_random = Reader()\n",
    "\n",
    "data_random_split = Dataset.load_from_df(df_ratings_subset[['userId', 'movieId', 'rating']], reader_random)\n",
    "\n",
    "svd_random = SVD()\n",
    "\n",
    "cross_validate(svd_random, data_random_split, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The superior performance of the random split (model RMSE 0.7847) suggests that it may offer a more balanced and varied dataset for both training and testing phases, potentially leading to a model that is better at generalizing across the entire dataset. \n",
    "\n",
    "Yet, as already mentioned for a real-world recommender systems, a temporal split is often preferred to account for evolving preferences and trends over time. For a movie recommender system, especially one like DreamStream that might experience frequent updates to its movie catalog and shifts in user preferences, we  suggest a temporal split. This approach acknowledges the evolving nature of both movies and user tastes, preparing the system to adapt to real-world scenarios more effectively. It also allows the system to better handle cold start problems with new releases. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17af3ef4a152cce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get back to our temporal split and try to optimize our model using a GridSearch to find the best combination of hyperparameter for the model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "772fca0e31f76bc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9688\n",
      "RMSE: 0.9688\n",
      "RMSE: 0.9692\n",
      "RMSE: 0.9687\n",
      "RMSE: 0.9719\n",
      "RMSE: 0.9701\n",
      "RMSE: 0.9725\n",
      "RMSE: 0.9699\n",
      "Best RMSE score obtained:  0.9687174189065906\n",
      "Best parameters:  {'lr_all': 0.005, 'n_epochs': 20, 'n_factors': 100, 'reg_all': 0.05}\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "data = Dataset.load_from_df(train_data[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "testset = list(zip(test_data['userId'].values, test_data['movieId'].values, test_data['rating'].values))\n",
    "\n",
    "# our grid of parameters\n",
    "param_grid = {'n_factors': [50, 100],  # Number of factors\n",
    "              'n_epochs': [20],         # Number of iterations\n",
    "              'lr_all': [0.005, 0.01],      # Learning rate\n",
    "              'reg_all': [0.02, 0.05]}      # Regularization term\n",
    "\n",
    "svd = SVD()\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Loop through parameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    svd = SVD(**params)\n",
    "    svd.fit(trainset)\n",
    "\n",
    "\n",
    "    predictions = svd.test(testset)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "\n",
    "    # Update best RMSE and parameters if necessary\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best RMSE score obtained: \", best_rmse)\n",
    "print(\"Best parameters: \", best_params)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:15:28.330195Z",
     "start_time": "2024-04-02T15:13:53.991152Z"
    }
   },
   "id": "75b157c6c93dba85",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best RMSE score obtained is 0.9687 with the following parameters: \n",
    "\n",
    "lr_all: 0.005, n_epochs: 20, n_factors: 100, reg_all: 0.05\n",
    "\n",
    "This is a slightly  better RMSE score as we obtained with the default parameters (RMSE 0.9698). With higher computational power and time, we could further optimize the model by testing more hyperparameters and combinations. At this stage we will stick with the selected parameters from our GridSearch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f5d0e127b653a8"
  },
  {
   "cell_type": "markdown",
   "id": "3e9f81f6",
   "metadata": {},
   "source": [
    "Let us now train the best version of our model on the full subset and predict the top ten recommendations for a selected user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba4fd623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:24:36.058197Z",
     "start_time": "2024-04-02T15:24:22.711735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<surprise.prediction_algorithms.matrix_factorization.SVD at 0xbc4baecd0>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = SVD(**best_params)\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prediction for user: 14204"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9e6aa3a4bc17b2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommended movies for User 14204:\n",
      "Rank 1: Movie ID 93040, Predicted Rating: 4.209856235490476\n",
      "Rank 2: Movie ID 137904, Predicted Rating: 4.120377594522206\n",
      "Rank 3: Movie ID 159817, Predicted Rating: 4.104339770487309\n",
      "Rank 4: Movie ID 8484, Predicted Rating: 4.071723551405211\n",
      "Rank 5: Movie ID 3677, Predicted Rating: 4.056649558591241\n",
      "Rank 6: Movie ID 97673, Predicted Rating: 4.046851602150687\n",
      "Rank 7: Movie ID 105250, Predicted Rating: 4.038531369323463\n",
      "Rank 8: Movie ID 82143, Predicted Rating: 4.027010837440242\n",
      "Rank 9: Movie ID 69830, Predicted Rating: 4.01387029733196\n",
      "Rank 10: Movie ID 54229, Predicted Rating: 4.011497108180964\n"
     ]
    }
   ],
   "source": [
    "selected_user_id = 14204\n",
    "rated_movie_ids = df_ratings_subset[df_ratings_subset['userId'] == selected_user_id]['movieId'].unique()\n",
    "all_movie_ids = df_ratings_subset['movieId'].unique()\n",
    "\n",
    "# Predict ratings for all movies that the selected user has not rated yet\n",
    "predicted_unrated_movies = []\n",
    "for movie_id in all_movie_ids:\n",
    "    if movie_id not in rated_movie_ids:\n",
    "        prediction = svd.predict(uid=selected_user_id, iid=movie_id)\n",
    "        predicted_unrated_movies.append((movie_id, prediction.est))\n",
    "\n",
    "# sorting\n",
    "sorted_predicted_unrated_movies = sorted(predicted_unrated_movies, key=lambda x: x[1], reverse=True)\n",
    "top_10_unrated_movies = sorted_predicted_unrated_movies[:10]\n",
    "\n",
    "# Top 10 predicted ratings for the selected user\n",
    "print(f\"Top 10 recommended movies for User {selected_user_id}:\")\n",
    "for i, (movie_id, predicted_rating) in enumerate(top_10_unrated_movies, start=1):\n",
    "    print(f\"Rank {i}: Movie ID {movie_id}, Predicted Rating: {predicted_rating}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:24:38.075595Z",
     "start_time": "2024-04-02T15:24:37.864724Z"
    }
   },
   "id": "5e01dd8734081183",
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
